General description or introduction of the problem and your solution:<br>
&emsp;This midterm project, I implemented binary search algorithm for TLM platform and HLS. Binary search is a divide and conquer algorithm to find a specific value in a sorted array. I decomposed the project into software and hardware parts, the software part is the testbench and the hardware part is the binary search hardware I implemented. The hardware part of binary search is implemented as 8 numbers array binary search. For simplicity, I use a char variable to store input array size, thus, the input size has maximum number 255, so the input array size can be 1 to 255. Of course, the input size can be larger by some small changes in the source code. The input and output example is as following:<br>
![image](https://user-images.githubusercontent.com/101209771/165588790-9cfcc617-db48-400c-96f1-3c5b30d3747b.png)
![image](https://user-images.githubusercontent.com/101209771/165589030-e839f931-ce6b-4472-b324-f8068a2381cb.png)<br>
block diagram:<br>
![image](https://user-images.githubusercontent.com/101209771/165601832-f2422401-b221-4a54-8e31-90a5b97a7082.png)<br>

Implementation details (data structure, flows and algorithms):<br>
&emsp;Since the input size can be customizable, and the hardware part for binary search is designed for 8 numbers bianry search. So, when the input size is larger than 8 numbers, the input array will be separated into different sets with fixed size 8 numbers in order to fit the hardware. The input array is decomposed into a fixed-sized slice (8 numbers) and store it locally in the hardware module to do the binary search algorithm. For testbench part, the input array will be transfer to hardware module in order of sets. Also, the total array size will be sent to hardware part to calculate how many iterations are needed. For the hardware part, 8 numbers of binary search will be done every iteration and after all iterations are finsihed, hardware module will transfer whether the value is found and if so, what are its index. In TLM part, the operation can transfer to the kernel hardware via register interface (memory map).<br>
![image](https://user-images.githubusercontent.com/101209771/165591634-ea5318b9-ec1f-40af-822d-61f28adc9eea.png)<br>
Additional features of your design and models:<br>
In the beginning, my midterm project is designed to solve fixed size problem. Thus, it is not useful since the problem size may change. So, I changed my design and the way that the data transfer between testbench and hardware module. It makes my design more general and be useful on different problem size.<br>
Experimental results (on different benchmarks and settings):<br>
For TLM platform, the total simulation time is 417ns when the input array size is 112 numbers. And for the same input, the total simulation time for behavioral in HLS is 2370ns. And the total simulation time is 255ns when the input array size is 64 numbers. And for the same input, the total simulation time for behavioral in HLS is 1410ns. We can found that in HLS, the simulation time is larger since the timing is more accurate when doing HLS behavioral simulation.<br>
For HLS, I have tried different HLS directives to compare their synthesis result, and the comparison is shown in following sheet:<br>
![image](https://user-images.githubusercontent.com/101209771/165596343-128a0a27-624f-4b8b-8e40-ff61e62d997a.png)
![image](https://user-images.githubusercontent.com/101209771/165596400-e35031ba-6345-41d3-90b6-1637a703e474.png)<br>
We can found that the total latency is about two times larger when the input problem size is about two times larger. And by DPA directive, both the area and latency drop comparing to basic HLS. And using the unroll directive makes the total latency drops about (4 * input size / 8) and the area is almost unchanged. I think the reason is that in binary search, the iterations exist dependencies, thus, using HLS unroll directive expand the iterations into one iteration but really unroll it. Besides, I have tried to unroll the loops manually by two, and the result shows that the latency for each loop needs two times larger so the result remains the same. For binary search, it is hard to do loop unrolling since the numbers next iteration need come from previous iteration. Also, I have tried to apply HLS pipeline when doing synthesis. It turns out that the total latency can drop obviously, however, the result may have mistake. I think the reason is that when doing pipelining, the dependency between sets go wrong and make the result wrong.<br>
Discussions and conclusions:<br>
I think it is quite hard to do pipelining or unrolling in many case of application because there are dependencies between the iterations. In most of the cases, the dependencies are the difficult part to do optimization by loop unrolling. By implementing this midterm project, I have further understand how different HLS directives work and what the result may be. It is quite interesting that by some optimization, it is possible to make the area of synthesis result double but make the toatl latency one third. So, it is improtant to attempt different synthesis strategy on a design to find the most acceptable result.<br>
